{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка датасета - 2 часть, генерация по файлам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорты и сетап окружения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T09:36:35.720883Z",
     "start_time": "2025-04-06T09:36:34.354367Z"
    }
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "import aiopath\n",
    "import sys\n",
    "import time\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "ROOT_DIR = str((await aiopath.AsyncPath.cwd()).parent)\n",
    "os.chdir(ROOT_DIR)\n",
    "DATA_DIR = ROOT_DIR+'/data'\n",
    "REPOS_DIR = DATA_DIR+'/repos'\n",
    "if ROOT_DIR not in sys.path:\n",
    "    sys.path.append(ROOT_DIR)\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:128'\n",
    "\n",
    "from evaluating.generator import make_model_with_tokenizer, generate_completions\n",
    "from scoring.scoring import Scorer\n",
    "\n",
    "gvm_root = os.environ.get('GVM_ROOT')\n",
    "os.environ['PATH'] = f\"{gvm_root}/bin:{gvm_root}/pkgsets/go1.24.2/global/bin:{gvm_root}/gos/go1.24.2/bin:{gvm_root}/pkgsets/go1.24.2/global/overlay/bin:{os.environ['PATH']}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В части 1 [dataset_0](./dataset_0.ipynb) подготовлен датасет test_candidates_ds с отфильтрованными файлами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_candidates_ds = datasets.load_from_disk(DATA_DIR+'/test_candidates_ds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Записываем промпты в датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44567ecc21614367b53397cec0e41b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/33251 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['project_path', 'relative_package_path', 'relative_file_path', 'prompt'],\n",
      "    num_rows: 33251\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'project_path': '766dc882d779f07821bde740ce49802f67ae42b3/backend/',\n",
       " 'relative_package_path': 'controllers/',\n",
       " 'relative_file_path': 'controllers/controllers.go',\n",
       " 'prompt': [{'content': '\\nYou are an expert programmer. \\nYou should only return output test file containing working code.\\nThe user is going to give you code and would like to have unit tests for the first file.\\nAll the other files are just dependencies to give you context of all the possible test cases to produce.\\nCover all possible inputs and their respective outputs using tests.\\nEach subtest must be wrapped into t.Run\\n',\n",
       "   'role': 'system'},\n",
       "  {'content': 'package controllers\\n\\nimport (\\n\\t\"encoding/json\"\\n\\t\"fmt\"\\n\\techo \"github.com/labstack/echo/v4\"\\n\\t\"golang.org/x/net/websocket\"\\n\\t\"stream/models\"\\n)\\n\\n// Controller interface has two methods\\ntype Controller interface {\\n\\t// Homecontroller renders initial home page\\n\\tHomeController(e echo.Context) error\\n\\n\\t// StreamController responds with live cpu status over websocket\\n\\tStreamController(e echo.Context) error\\n}\\n\\ntype controller struct {\\n}\\n\\nfunc NewController() Controller {\\n\\treturn &controller{}\\n}\\n\\nvar model models.Model\\n\\n// Initializes the models\\nfunc Init(){\\n\\tmodel = models.NewModel()\\n}\\n\\n\\n\\nfunc (c *controller) HomeController(e echo.Context) error {\\n\\treturn e.File(\"views/index.html\")\\n}\\n\\nfunc (c *controller) StreamController(e echo.Context) error {\\n\\n\\twebsocket.Handler(func(ws *websocket.Conn) {\\n\\t\\tdefer ws.Close()\\n\\t\\tstatus, err := model.GetLiveCpuUsage()\\n\\t\\tif err != nil {\\n\\t\\t\\tfmt.Println(err)\\n\\t\\t\\treturn\\n\\t\\t}\\n\\t\\tfor {\\n\\t\\t\\t// Write\\n\\t\\t\\tnewVal := <- status\\n\\t\\t\\tjsonResponse, _ := json.Marshal(newVal)\\n\\t\\t\\terr := websocket.Message.Send(ws, fmt.Sprintln(string(jsonResponse)))\\n\\t\\t\\tif err != nil {\\n\\t\\t\\t\\tfmt.Println(err)\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}).ServeHTTP(e.Response(), e.Request())\\n\\treturn nil\\n}',\n",
       "   'role': 'user'}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message = \"\"\"\n",
    "You are an expert programmer. \n",
    "You should only return output test file containing working code.\n",
    "The user is going to give you code and would like to have unit tests for the first file.\n",
    "All the other files are just dependencies to give you context of all the possible test cases to produce.\n",
    "Cover all possible inputs and their respective outputs using tests.\n",
    "Each subtest must be wrapped into t.Run\n",
    "\"\"\"\n",
    "\n",
    "def get_prompt(row) -> str:\n",
    "    file_path = '../data/repos/'+row['project_path']+row['relative_project_path']\n",
    "    f = open(file_path, 'r')\n",
    "    file_content = f.read()\n",
    "    f.close()\n",
    "\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": f\"{file_content}\"}\n",
    "    ]\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def finalize_row(row) -> dict:\n",
    "    return {\n",
    "        'project_path': row['project_path'],\n",
    "        'relative_package_path': row['relative_go_package'],\n",
    "        'relative_file_path': row['relative_project_path'],\n",
    "        'prompt': get_prompt(row)\n",
    "    }\n",
    "\n",
    "by_file_ds = test_candidates_ds.map(finalize_row, num_proc=32).select_columns(['project_path', 'relative_package_path', 'relative_file_path', 'prompt'])\n",
    "\n",
    "by_file_ds.save_to_disk('../data/by_file_ds')\n",
    "\n",
    "print(by_file_ds)\n",
    "by_file_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерируем completions исходной моделью `deepseek-ai/deepseek-coder-1.3b-instruct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_file_ds = datasets.load_from_disk(DATA_DIR+'/by_file_ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28eb470df64843b8b6e094f6cb03f68c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['project_path', 'relative_package_path', 'relative_file_path', 'prompt'],\n",
      "    num_rows: 500\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'project_path': '8844bcf1e44c80eb6c94e96c1c0466177f3bba94/',\n",
       " 'relative_package_path': 'packages/testutil/testchain/',\n",
       " 'relative_file_path': 'packages/testutil/testchain/mock_nodeconn.go',\n",
       " 'prompt': [{'content': '\\nYou are an expert programmer. \\nYou should only return output test file containing working code.\\nThe user is going to give you code and would like to have unit tests for the first file.\\nAll the other files are just dependencies to give you context of all the possible test cases to produce.\\nCover all possible inputs and their respective outputs using tests.\\nEach subtest must be wrapped into t.Run\\n',\n",
       "   'role': 'system'},\n",
       "  {'content': 'package testchain\\n\\nimport (\\n\\t\"github.com/iotaledger/goshimmer/packages/ledgerstate\"\\n)\\n\\ntype MockedNodeConn struct {\\n\\tid                              string\\n\\tonPullBacklog                   func(addr *ledgerstate.AliasAddress)\\n\\tonPullState                     func(addr *ledgerstate.AliasAddress)\\n\\tonPullConfirmedTransaction      func(addr ledgerstate.Address, txid ledgerstate.TransactionID)\\n\\tonPullTransactionInclusionState func(addr ledgerstate.Address, txid ledgerstate.TransactionID)\\n\\tonPullConfirmedOutput           func(addr ledgerstate.Address, outputID ledgerstate.OutputID)\\n\\tonPostTransaction               func(tx *ledgerstate.Transaction)\\n}\\n\\nfunc NewMockedNodeConnection(id string) *MockedNodeConn {\\n\\treturn &MockedNodeConn{id: id}\\n}\\n\\nfunc (m *MockedNodeConn) ID() string {\\n\\treturn m.id\\n}\\n\\nfunc (m *MockedNodeConn) PullBacklog(addr *ledgerstate.AliasAddress) {\\n\\tm.onPullBacklog(addr)\\n}\\n\\nfunc (m *MockedNodeConn) PullState(addr *ledgerstate.AliasAddress) {\\n\\tm.onPullState(addr)\\n}\\n\\nfunc (m *MockedNodeConn) PullConfirmedTransaction(addr ledgerstate.Address, txid ledgerstate.TransactionID) {\\n\\tm.onPullConfirmedTransaction(addr, txid)\\n}\\n\\nfunc (m *MockedNodeConn) PullTransactionInclusionState(addr ledgerstate.Address, txid ledgerstate.TransactionID) {\\n\\tm.onPullTransactionInclusionState(addr, txid)\\n}\\n\\nfunc (m *MockedNodeConn) PullConfirmedOutput(addr ledgerstate.Address, outputID ledgerstate.OutputID) {\\n\\tm.onPullConfirmedOutput(addr, outputID)\\n}\\n\\nfunc (m *MockedNodeConn) PostTransaction(tx *ledgerstate.Transaction) {\\n\\tm.onPostTransaction(tx)\\n}\\n\\nfunc (m *MockedNodeConn) OnPullBacklog(f func(addr *ledgerstate.AliasAddress)) {\\n\\tm.onPullBacklog = f\\n}\\n\\nfunc (m *MockedNodeConn) OnPullState(f func(addr *ledgerstate.AliasAddress)) {\\n\\tm.onPullState = f\\n}\\n\\nfunc (m *MockedNodeConn) OnPullConfirmedTransaction(f func(addr ledgerstate.Address, txid ledgerstate.TransactionID)) {\\n\\tm.onPullConfirmedTransaction = f\\n}\\n\\nfunc (m *MockedNodeConn) OnPullTransactionInclusionState(f func(addr ledgerstate.Address, txid ledgerstate.TransactionID)) {\\n\\tm.onPullTransactionInclusionState = f\\n}\\n\\nfunc (m *MockedNodeConn) OnPullConfirmedOutput(f func(addr ledgerstate.Address, outputID ledgerstate.OutputID)) {\\n\\tm.onPullConfirmedOutput = f\\n}\\n\\nfunc (m *MockedNodeConn) OnPostTransaction(f func(tx *ledgerstate.Transaction)) {\\n\\tm.onPostTransaction = f\\n}\\n',\n",
       "   'role': 'user'}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "by_file_test_ds = by_file_ds.shuffle().take(500)\n",
    "\n",
    "by_file_test_ds.save_to_disk('../data/by_file_test_ds')\n",
    "\n",
    "print(by_file_test_ds)\n",
    "by_file_test_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_file_test_ds = datasets.load_from_disk(DATA_DIR+'/by_file_test_ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 03:26:59.243822: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747614419.373649    7231 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747614419.411928    7231 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747614419.701020    7231 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747614419.701056    7231 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747614419.701060    7231 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747614419.701064    7231 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-19 03:26:59.728427: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f7bfa6aaa4477fbb92c00701a6e4d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9a9fb88d204ebb99e2c1437075604b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['project_path', 'relative_package_path', 'relative_file_path', 'prompt', 'generate_time', 'completion'],\n",
      "    num_rows: 500\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'project_path': '8844bcf1e44c80eb6c94e96c1c0466177f3bba94/',\n",
       " 'relative_package_path': 'packages/testutil/testchain/',\n",
       " 'relative_file_path': 'packages/testutil/testchain/mock_nodeconn.go',\n",
       " 'prompt': [{'content': '\\nYou are an expert programmer. \\nYou should only return output test file containing working code.\\nThe user is going to give you code and would like to have unit tests for the first file.\\nAll the other files are just dependencies to give you context of all the possible test cases to produce.\\nCover all possible inputs and their respective outputs using tests.\\nEach subtest must be wrapped into t.Run\\n',\n",
       "   'role': 'system'},\n",
       "  {'content': 'package testchain\\n\\nimport (\\n\\t\"github.com/iotaledger/goshimmer/packages/ledgerstate\"\\n)\\n\\ntype MockedNodeConn struct {\\n\\tid                              string\\n\\tonPullBacklog                   func(addr *ledgerstate.AliasAddress)\\n\\tonPullState                     func(addr *ledgerstate.AliasAddress)\\n\\tonPullConfirmedTransaction      func(addr ledgerstate.Address, txid ledgerstate.TransactionID)\\n\\tonPullTransactionInclusionState func(addr ledgerstate.Address, txid ledgerstate.TransactionID)\\n\\tonPullConfirmedOutput           func(addr ledgerstate.Address, outputID ledgerstate.OutputID)\\n\\tonPostTransaction               func(tx *ledgerstate.Transaction)\\n}\\n\\nfunc NewMockedNodeConnection(id string) *MockedNodeConn {\\n\\treturn &MockedNodeConn{id: id}\\n}\\n\\nfunc (m *MockedNodeConn) ID() string {\\n\\treturn m.id\\n}\\n\\nfunc (m *MockedNodeConn) PullBacklog(addr *ledgerstate.AliasAddress) {\\n\\tm.onPullBacklog(addr)\\n}\\n\\nfunc (m *MockedNodeConn) PullState(addr *ledgerstate.AliasAddress) {\\n\\tm.onPullState(addr)\\n}\\n\\nfunc (m *MockedNodeConn) PullConfirmedTransaction(addr ledgerstate.Address, txid ledgerstate.TransactionID) {\\n\\tm.onPullConfirmedTransaction(addr, txid)\\n}\\n\\nfunc (m *MockedNodeConn) PullTransactionInclusionState(addr ledgerstate.Address, txid ledgerstate.TransactionID) {\\n\\tm.onPullTransactionInclusionState(addr, txid)\\n}\\n\\nfunc (m *MockedNodeConn) PullConfirmedOutput(addr ledgerstate.Address, outputID ledgerstate.OutputID) {\\n\\tm.onPullConfirmedOutput(addr, outputID)\\n}\\n\\nfunc (m *MockedNodeConn) PostTransaction(tx *ledgerstate.Transaction) {\\n\\tm.onPostTransaction(tx)\\n}\\n\\nfunc (m *MockedNodeConn) OnPullBacklog(f func(addr *ledgerstate.AliasAddress)) {\\n\\tm.onPullBacklog = f\\n}\\n\\nfunc (m *MockedNodeConn) OnPullState(f func(addr *ledgerstate.AliasAddress)) {\\n\\tm.onPullState = f\\n}\\n\\nfunc (m *MockedNodeConn) OnPullConfirmedTransaction(f func(addr ledgerstate.Address, txid ledgerstate.TransactionID)) {\\n\\tm.onPullConfirmedTransaction = f\\n}\\n\\nfunc (m *MockedNodeConn) OnPullTransactionInclusionState(f func(addr ledgerstate.Address, txid ledgerstate.TransactionID)) {\\n\\tm.onPullTransactionInclusionState = f\\n}\\n\\nfunc (m *MockedNodeConn) OnPullConfirmedOutput(f func(addr ledgerstate.Address, outputID ledgerstate.OutputID)) {\\n\\tm.onPullConfirmedOutput = f\\n}\\n\\nfunc (m *MockedNodeConn) OnPostTransaction(f func(tx *ledgerstate.Transaction)) {\\n\\tm.onPostTransaction = f\\n}\\n',\n",
       "   'role': 'user'}],\n",
       " 'generate_time': 20.40785813331604,\n",
       " 'completion': 'Here is a simple example of how you can write unit tests for the `MockedNodeConn` struct using the `testing` package in Go.\\n\\n```go\\npackage testchain\\n\\nimport (\\n\\t\"testing\"\\n\\t\"github.com/iotaledger/goshimmer/packages/ledgerstate\"\\n)\\n\\nfunc TestMockedNodeConn(t *testing.T) {\\n\\tmockedNodeConn := NewMockedNodeConnection(\"testID\")\\n\\n\\t// Test ID method\\n\\tif mockedNodeConn.ID() != \"testID\" {\\n\\t\\tt.Errorf(\"Expected ID to be \\'testID\\', got %s\", mockedNodeConn.ID())\\n\\t}\\n\\n\\t// Test PullBacklog method\\n\\texpectedAddr := &ledgerstate.AliasAddress{}\\n\\tmockedNodeConn.PullBacklog(expectedAddr)\\n\\tif mockedNodeConn.onPullBacklog == nil {\\n\\t\\tt.Errorf(\"Expected onPullBacklog to be set, got nil\")\\n\\t} else if mockedNodeConn.onPullBacklog(expectedAddr) != nil {\\n\\t\\tt.Errorf(\"onPullBacklog should not return anything\")\\n\\t}\\n\\n\\t// Test PullState method\\n\\texpectedAddr = &ledgerstate.AliasAddress{}\\n\\tmockedNodeConn.PullState(expectedAddr)\\n\\tif mockedNodeConn.onPullState == nil {\\n\\t\\tt.Errorf(\"Expected onPullState to be set, got nil\")\\n\\t} else if mockedNodeConn.onPullState(expectedAddr) != nil {\\n\\t\\tt.Errorf(\"onPullState should not return anything\")\\n\\t}\\n\\n\\t// Test PullConfirmedTransaction method\\n\\texpectedAddr = ledgerstate.Address{}\\n\\texpectedTxID := ledgerstate.TransactionID{}\\n\\tmockedNodeConn.PullConfirmedTransaction(expectedAddr, expectedTxID)\\n\\tif mockedNodeConn.onPullConfirmedTransaction == nil {\\n\\t\\tt.Errorf(\"Expected onPullConfirmedTransaction to be set, got nil\")\\n\\t} else if mockedNodeConn.onPullConfirmedTransaction(expectedAddr, expectedTxID) != nil {\\n\\t\\tt.Errorf(\"onPullConfirmedTransaction should not return anything\")\\n\\t}\\n\\n\\t// Test PullTransactionInclusionState method\\n\\texpectedAddr = ledgerstate.Address{}\\n\\texpectedTxID = ledgerstate.TransactionID{}\\n\\tmockedNodeConn.PullTransactionInclusionState(expectedAddr, expectedTxID)\\n\\tif mockedNodeConn.onPullTransactionInclusionState == nil {\\n\\t\\tt.Errorf(\"Expected onPullTransactionInclusionState to be set, got nil\")\\n\\t} else if mockedNodeConn.onPullTransactionInclusionState(expectedAddr, expectedTxID) != nil {\\n\\t\\tt.Errorf(\"onPullTransactionInclusionState should not return anything\")\\n\\t}\\n\\n\\t// Test PullConfirmedOutput method\\n\\texpectedAddr = ledgerstate.Address{}\\n\\texpectedOutputID := ledgerstate.OutputID{}\\n\\tmockedNodeConn.PullConfirmedOutput(expectedAddr, expectedOutputID)\\n\\tif mockedNodeConn.onPullConfirmedOutput == nil {\\n\\t\\tt.Errorf(\"Expected onPullConfirmedOutput to be set, got nil\")\\n\\t} else if mockedNodeConn.onPullConfirmedOutput(expectedAddr, expectedOutputID) != nil {\\n\\t\\tt.Errorf(\"onPullConfirmedOutput should not return anything\")\\n\\t}\\n\\n\\t// Test PostTransaction method\\n\\texpectedTx := &ledgerstate.Transaction{}\\n\\tmockedNodeConn.PostTransaction(expectedTx)\\n\\tif mockedNodeConn.onPostTransaction == nil {\\n\\t\\tt.Errorf(\"Expected onPostTransaction to be set, got nil\")\\n\\t} else if mockedNodeConn.onPostTransaction(expectedTx) != nil {\\n\\t\\tt.Errorf(\"onPostTransaction should not return anything\")\\n\\t}\\n}\\n```\\n\\nThis test suite covers the basic functionality of the `MockedNodeConn` struct. It checks if the methods are correctly setting and unsetting the corresponding callback functions, and if they are correctly calling the callback functions with the expected arguments.\\n'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer, model = make_model_with_tokenizer('original')\n",
    "\n",
    "def generate_completion_for_row(row: dict) -> dict:\n",
    "    start = time.time()\n",
    "    completion = generate_completions(tokenizer, model, [row['prompt']])[0]\n",
    "    return {\n",
    "        'generate_time': time.time()-start,\n",
    "        'completion': completion,\n",
    "    }\n",
    "\n",
    "by_file_test_with_completions_ds = by_file_test_ds.map(generate_completion_for_row, num_proc=1)\n",
    "\n",
    "by_file_test_with_completions_ds.save_to_disk(DATA_DIR+'/by_file_test_with_completions_ds')\n",
    "del model, tokenizer\n",
    "\n",
    "print(by_file_test_with_completions_ds)\n",
    "by_file_test_with_completions_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0616b5b81640dc842b81906fd33d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f5a7bf4a9c4f44b42f5a6e61a8fc47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c321e61abf849a185b44653dec1dd02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     24\u001b[39m     completion = generate_completions(tokenizer, model, [row[\u001b[33m'\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m'\u001b[39m]])[\u001b[32m0\u001b[39m]\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     26\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mgenerate_time\u001b[39m\u001b[33m'\u001b[39m: time.time()-start,\n\u001b[32m     27\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcompletion\u001b[39m\u001b[33m'\u001b[39m: completion,\n\u001b[32m     28\u001b[39m     }\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m by_file_test_with_completions_qwen_ds = \u001b[43mby_file_test_ds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerate_completion_for_row\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m by_file_test_with_completions_qwen_ds.save_to_disk(DATA_DIR+\u001b[33m'\u001b[39m\u001b[33m/by_file_test_with_completions_qwen_ds\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m model, tokenizer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:557\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    550\u001b[39m self_format = {\n\u001b[32m    551\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    552\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    553\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    555\u001b[39m }\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3079\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3073\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3074\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[32m   3075\u001b[39m         unit=\u001b[33m\"\u001b[39m\u001b[33m examples\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3076\u001b[39m         total=pbar_total,\n\u001b[32m   3077\u001b[39m         desc=desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mMap\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3078\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m3079\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3501\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[39m\n\u001b[32m   3499\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batched:\n\u001b[32m   3500\u001b[39m     _time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m3501\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_iterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3502\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mupdate_data\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3503\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3475\u001b[39m, in \u001b[36mDataset._map_single.<locals>.iter_outputs\u001b[39m\u001b[34m(shard_iterable)\u001b[39m\n\u001b[32m   3473\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3474\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[32m-> \u001b[39m\u001b[32m3475\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3398\u001b[39m, in \u001b[36mDataset._map_single.<locals>.apply_function\u001b[39m\u001b[34m(pa_inputs, indices, offset)\u001b[39m\n\u001b[32m   3396\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[32m   3397\u001b[39m inputs, fn_args, additional_args, fn_kwargs = prepare_inputs(pa_inputs, indices, offset=offset)\n\u001b[32m-> \u001b[39m\u001b[32m3398\u001b[39m processed_inputs = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3399\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mgenerate_completion_for_row\u001b[39m\u001b[34m(row)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_completion_for_row\u001b[39m(row: \u001b[38;5;28mdict\u001b[39m) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m     23\u001b[39m     start = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     completion = \u001b[43mgenerate_completions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     26\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mgenerate_time\u001b[39m\u001b[33m'\u001b[39m: time.time()-start,\n\u001b[32m     27\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcompletion\u001b[39m\u001b[33m'\u001b[39m: completion,\n\u001b[32m     28\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/evaluating/generator.py:56\u001b[39m, in \u001b[36mgenerate_completions\u001b[39m\u001b[34m(tokenizer, model, prompts)\u001b[39m\n\u001b[32m     54\u001b[39m model.generation_config.pad_token_id = tokenizer.pad_token_id\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# with torch.no_grad()\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# top_p=0.95,\u001b[39;49;00m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m outputs = [output[model_inputs[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m].shape[\u001b[32m1\u001b[39m]:] \u001b[38;5;28;01mfor\u001b[39;00m i, output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(outputs)]\n\u001b[32m     62\u001b[39m completions = tokenizer.batch_decode(outputs, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2597\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2589\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2590\u001b[39m         input_ids=input_ids,\n\u001b[32m   2591\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2592\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2593\u001b[39m         **model_kwargs,\n\u001b[32m   2594\u001b[39m     )\n\u001b[32m   2596\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2597\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2598\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2599\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2600\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2602\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2603\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2604\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2605\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2607\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2608\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2609\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2610\u001b[39m         input_ids=input_ids,\n\u001b[32m   2611\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2612\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2613\u001b[39m         **model_kwargs,\n\u001b[32m   2614\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:3560\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3558\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3559\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3560\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3562\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3563\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3564\u001b[39m     outputs,\n\u001b[32m   3565\u001b[39m     model_kwargs,\n\u001b[32m   3566\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3567\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:969\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    966\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    971\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:703\u001b[39m, in \u001b[36mQwen2ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    698\u001b[39m output_hidden_states = (\n\u001b[32m    699\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    700\u001b[39m )\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    716\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    717\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:969\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    966\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    971\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:436\u001b[39m, in \u001b[36mQwen2Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    434\u001b[39m     all_hidden_states += (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    448\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/transformers/modeling_layers.py:48\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.gradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:257\u001b[39m, in \u001b[36mQwen2DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    254\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    256\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m hidden_states, self_attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    268\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    270\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:169\u001b[39m, in \u001b[36mQwen2Attention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n\u001b[32m    168\u001b[39m     cache_kwargs = {\u001b[33m\"\u001b[39m\u001b[33msin\u001b[39m\u001b[33m\"\u001b[39m: sin, \u001b[33m\"\u001b[39m\u001b[33mcos\u001b[39m\u001b[33m\"\u001b[39m: cos, \u001b[33m\"\u001b[39m\u001b[33mcache_position\u001b[39m\u001b[33m\"\u001b[39m: cache_position}\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m     key_states, value_states = \u001b[43mpast_key_value\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m sliding_window = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    173\u001b[39m     \u001b[38;5;28mself\u001b[39m.config.use_sliding_window\n\u001b[32m    174\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.config, \u001b[33m\"\u001b[39m\u001b[33msliding_window\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    175\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layer_idx >= \u001b[38;5;28mself\u001b[39m.config.max_window_layers\n\u001b[32m    176\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/transformers/cache_utils.py:545\u001b[39m, in \u001b[36mDynamicCache.update\u001b[39m\u001b[34m(self, key_states, value_states, layer_idx, cache_kwargs)\u001b[39m\n\u001b[32m    543\u001b[39m         \u001b[38;5;28mself\u001b[39m.value_cache[layer_idx] = value_states\n\u001b[32m    544\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m545\u001b[39m         \u001b[38;5;28mself\u001b[39m.key_cache[layer_idx] = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    546\u001b[39m         \u001b[38;5;28mself\u001b[39m.value_cache[layer_idx] = torch.cat([\u001b[38;5;28mself\u001b[39m.value_cache[layer_idx], value_states], dim=-\u001b[32m2\u001b[39m)\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.key_cache[layer_idx], \u001b[38;5;28mself\u001b[39m.value_cache[layer_idx]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# qwen\n",
    "\n",
    "model_name = 'Qwen/Qwen2.5-Coder-1.5B'\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    #torch_dtype=torch.float16,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    #use_cache=False,\n",
    "    #low_cpu_mem_usage=True,\n",
    "    #local_files_only=True,\n",
    "    # quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "\n",
    ")\n",
    "\n",
    "compiled_model = torch.compile(model)\n",
    "\n",
    "def generate_completion_for_row(row: dict) -> dict:\n",
    "    start = time.time()\n",
    "    completion = generate_completions(tokenizer, model, [row['prompt']])[0]\n",
    "    return {\n",
    "        'generate_time': time.time()-start,\n",
    "        'completion': completion,\n",
    "    }\n",
    "\n",
    "by_file_test_with_completions_qwen_ds = by_file_test_ds.map(generate_completion_for_row, num_proc=1)\n",
    "\n",
    "by_file_test_with_completions_qwen_ds.save_to_disk(DATA_DIR+'/by_file_test_with_completions_qwen_ds')\n",
    "del model, tokenizer\n",
    "\n",
    "print(by_file_test_with_completions_qwen_ds)\n",
    "by_file_test_with_completions_qwen_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04337ca1ed349b180b63a7faa1506ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/53.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce60e10247244f9884c5ed0eed8db55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a6dd902ebc45fbb3af584549d321e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36fc561c71524599b51cbf246b532d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/934 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a877bad50eb46ebad0564c4e25076a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a51f1e2cc2c24d30a7fad97b465067ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/206 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60c3fb915504506b98eba3720188dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     26\u001b[39m     completion = generate_completions(tokenizer, model, [row[\u001b[33m'\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m'\u001b[39m]])[\u001b[32m0\u001b[39m]\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     28\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mgenerate_time\u001b[39m\u001b[33m'\u001b[39m: time.time()-start,\n\u001b[32m     29\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcompletion\u001b[39m\u001b[33m'\u001b[39m: completion,\n\u001b[32m     30\u001b[39m     }\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m by_file_test_with_completions_llama_ds = \u001b[43mby_file_test_ds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerate_completion_for_row\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m by_file_test_with_completions_llama_ds.save_to_disk(DATA_DIR+\u001b[33m'\u001b[39m\u001b[33m/by_file_test_with_completions_llama_ds\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m model, tokenizer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:557\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    550\u001b[39m self_format = {\n\u001b[32m    551\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    552\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    553\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    555\u001b[39m }\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3079\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3073\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3074\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[32m   3075\u001b[39m         unit=\u001b[33m\"\u001b[39m\u001b[33m examples\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3076\u001b[39m         total=pbar_total,\n\u001b[32m   3077\u001b[39m         desc=desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mMap\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3078\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m3079\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3501\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[39m\n\u001b[32m   3499\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batched:\n\u001b[32m   3500\u001b[39m     _time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m3501\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_iterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3502\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mupdate_data\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3503\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3475\u001b[39m, in \u001b[36mDataset._map_single.<locals>.iter_outputs\u001b[39m\u001b[34m(shard_iterable)\u001b[39m\n\u001b[32m   3473\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3474\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[32m-> \u001b[39m\u001b[32m3475\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:3398\u001b[39m, in \u001b[36mDataset._map_single.<locals>.apply_function\u001b[39m\u001b[34m(pa_inputs, indices, offset)\u001b[39m\n\u001b[32m   3396\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[32m   3397\u001b[39m inputs, fn_args, additional_args, fn_kwargs = prepare_inputs(pa_inputs, indices, offset=offset)\n\u001b[32m-> \u001b[39m\u001b[32m3398\u001b[39m processed_inputs = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3399\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mgenerate_completion_for_row\u001b[39m\u001b[34m(row)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_completion_for_row\u001b[39m(row: \u001b[38;5;28mdict\u001b[39m) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m     25\u001b[39m     start = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     completion = \u001b[43mgenerate_completions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     28\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mgenerate_time\u001b[39m\u001b[33m'\u001b[39m: time.time()-start,\n\u001b[32m     29\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcompletion\u001b[39m\u001b[33m'\u001b[39m: completion,\n\u001b[32m     30\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/evaluating/generator.py:56\u001b[39m, in \u001b[36mgenerate_completions\u001b[39m\u001b[34m(tokenizer, model, prompts)\u001b[39m\n\u001b[32m     54\u001b[39m model.generation_config.pad_token_id = tokenizer.pad_token_id\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# with torch.no_grad()\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# top_p=0.95,\u001b[39;49;00m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m outputs = [output[model_inputs[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m].shape[\u001b[32m1\u001b[39m]:] \u001b[38;5;28;01mfor\u001b[39;00m i, output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(outputs)]\n\u001b[32m     62\u001b[39m completions = tokenizer.batch_decode(outputs, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:2597\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2589\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2590\u001b[39m         input_ids=input_ids,\n\u001b[32m   2591\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2592\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2593\u001b[39m         **model_kwargs,\n\u001b[32m   2594\u001b[39m     )\n\u001b[32m   2596\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2597\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2598\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2599\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2600\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2602\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2603\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2604\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2605\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2607\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2608\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2609\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2610\u001b[39m         input_ids=input_ids,\n\u001b[32m   2611\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2612\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2613\u001b[39m         **model_kwargs,\n\u001b[32m   2614\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/transformers/generation/utils.py:3560\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3558\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3559\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3560\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3562\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3563\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3564\u001b[39m     outputs,\n\u001b[32m   3565\u001b[39m     model_kwargs,\n\u001b[32m   3566\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3567\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:969\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    966\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    971\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:688\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    683\u001b[39m output_hidden_states = (\n\u001b[32m    684\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    685\u001b[39m )\n\u001b[32m    687\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m688\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    701\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:969\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    966\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    971\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:453\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    451\u001b[39m     all_hidden_states += (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m453\u001b[39m layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/transformers/modeling_layers.py:48\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.gradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:308\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    305\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    307\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m hidden_states, self_attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    319\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    321\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:252\u001b[39m, in \u001b[36mLlamaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    250\u001b[39m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n\u001b[32m    251\u001b[39m     cache_kwargs = {\u001b[33m\"\u001b[39m\u001b[33msin\u001b[39m\u001b[33m\"\u001b[39m: sin, \u001b[33m\"\u001b[39m\u001b[33mcos\u001b[39m\u001b[33m\"\u001b[39m: cos, \u001b[33m\"\u001b[39m\u001b[33mcache_position\u001b[39m\u001b[33m\"\u001b[39m: cache_position}\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m     key_states, value_states = \u001b[43mpast_key_value\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m attention_interface: Callable = eager_attention_forward\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config._attn_implementation != \u001b[33m\"\u001b[39m\u001b[33meager\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/vkr/.venv/lib/python3.12/site-packages/transformers/cache_utils.py:545\u001b[39m, in \u001b[36mDynamicCache.update\u001b[39m\u001b[34m(self, key_states, value_states, layer_idx, cache_kwargs)\u001b[39m\n\u001b[32m    543\u001b[39m         \u001b[38;5;28mself\u001b[39m.value_cache[layer_idx] = value_states\n\u001b[32m    544\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m545\u001b[39m         \u001b[38;5;28mself\u001b[39m.key_cache[layer_idx] = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    546\u001b[39m         \u001b[38;5;28mself\u001b[39m.value_cache[layer_idx] = torch.cat([\u001b[38;5;28mself\u001b[39m.value_cache[layer_idx], value_states], dim=-\u001b[32m2\u001b[39m)\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.key_cache[layer_idx], \u001b[38;5;28mself\u001b[39m.value_cache[layer_idx]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# dophine\n",
    "\n",
    "model_name = 'cognitivecomputations/Dolphin3.0-Llama3.2-1B'\n",
    "#model_name = 'meta-llama/Llama-3.2-1B-Instruct'\n",
    "#model_name = 'bigcode/starcoder2-3b'\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    #torch_dtype=torch.float16,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    #use_cache=False,\n",
    "    #low_cpu_mem_usage=True,\n",
    "    #local_files_only=True,\n",
    "    # quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "\n",
    ")\n",
    "\n",
    "compiled_model = torch.compile(model)\n",
    "\n",
    "def generate_completion_for_row(row: dict) -> dict:\n",
    "    start = time.time()\n",
    "    completion = generate_completions(tokenizer, model, [row['prompt']])[0]\n",
    "    return {\n",
    "        'generate_time': time.time()-start,\n",
    "        'completion': completion,\n",
    "    }\n",
    "\n",
    "by_file_test_with_completions_dophine_ds = by_file_test_ds.map(generate_completion_for_row, num_proc=1)\n",
    "\n",
    "by_file_test_with_completions_dophine_ds.save_to_disk(DATA_DIR+'/by_file_test_with_completions_dophine_ds')\n",
    "del model, tokenizer\n",
    "\n",
    "print(by_file_test_with_completions_dophine_ds)\n",
    "by_file_test_with_completions_dophine_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Скорим результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_file_test_with_completions_ds = datasets.load_from_disk(DATA_DIR+'/by_file_test_with_completions_ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b6cab0e704d4f8d8b550d8211ff9712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e32608c9bd4478948ef9fbf737de76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5290fd8b54ca4809aae7860273b11b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['project_path', 'relative_package_path', 'relative_file_path', 'prompt', 'generate_time', 'completion', 'score_time', 'result', 'reward'],\n",
      "    num_rows: 500\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'project_path': '8844bcf1e44c80eb6c94e96c1c0466177f3bba94/',\n",
       " 'relative_package_path': 'packages/testutil/testchain/',\n",
       " 'relative_file_path': 'packages/testutil/testchain/mock_nodeconn.go',\n",
       " 'prompt': [{'content': '\\nYou are an expert programmer. \\nYou should only return output test file containing working code.\\nThe user is going to give you code and would like to have unit tests for the first file.\\nAll the other files are just dependencies to give you context of all the possible test cases to produce.\\nCover all possible inputs and their respective outputs using tests.\\nEach subtest must be wrapped into t.Run\\n',\n",
       "   'role': 'system'},\n",
       "  {'content': 'package testchain\\n\\nimport (\\n\\t\"github.com/iotaledger/goshimmer/packages/ledgerstate\"\\n)\\n\\ntype MockedNodeConn struct {\\n\\tid                              string\\n\\tonPullBacklog                   func(addr *ledgerstate.AliasAddress)\\n\\tonPullState                     func(addr *ledgerstate.AliasAddress)\\n\\tonPullConfirmedTransaction      func(addr ledgerstate.Address, txid ledgerstate.TransactionID)\\n\\tonPullTransactionInclusionState func(addr ledgerstate.Address, txid ledgerstate.TransactionID)\\n\\tonPullConfirmedOutput           func(addr ledgerstate.Address, outputID ledgerstate.OutputID)\\n\\tonPostTransaction               func(tx *ledgerstate.Transaction)\\n}\\n\\nfunc NewMockedNodeConnection(id string) *MockedNodeConn {\\n\\treturn &MockedNodeConn{id: id}\\n}\\n\\nfunc (m *MockedNodeConn) ID() string {\\n\\treturn m.id\\n}\\n\\nfunc (m *MockedNodeConn) PullBacklog(addr *ledgerstate.AliasAddress) {\\n\\tm.onPullBacklog(addr)\\n}\\n\\nfunc (m *MockedNodeConn) PullState(addr *ledgerstate.AliasAddress) {\\n\\tm.onPullState(addr)\\n}\\n\\nfunc (m *MockedNodeConn) PullConfirmedTransaction(addr ledgerstate.Address, txid ledgerstate.TransactionID) {\\n\\tm.onPullConfirmedTransaction(addr, txid)\\n}\\n\\nfunc (m *MockedNodeConn) PullTransactionInclusionState(addr ledgerstate.Address, txid ledgerstate.TransactionID) {\\n\\tm.onPullTransactionInclusionState(addr, txid)\\n}\\n\\nfunc (m *MockedNodeConn) PullConfirmedOutput(addr ledgerstate.Address, outputID ledgerstate.OutputID) {\\n\\tm.onPullConfirmedOutput(addr, outputID)\\n}\\n\\nfunc (m *MockedNodeConn) PostTransaction(tx *ledgerstate.Transaction) {\\n\\tm.onPostTransaction(tx)\\n}\\n\\nfunc (m *MockedNodeConn) OnPullBacklog(f func(addr *ledgerstate.AliasAddress)) {\\n\\tm.onPullBacklog = f\\n}\\n\\nfunc (m *MockedNodeConn) OnPullState(f func(addr *ledgerstate.AliasAddress)) {\\n\\tm.onPullState = f\\n}\\n\\nfunc (m *MockedNodeConn) OnPullConfirmedTransaction(f func(addr ledgerstate.Address, txid ledgerstate.TransactionID)) {\\n\\tm.onPullConfirmedTransaction = f\\n}\\n\\nfunc (m *MockedNodeConn) OnPullTransactionInclusionState(f func(addr ledgerstate.Address, txid ledgerstate.TransactionID)) {\\n\\tm.onPullTransactionInclusionState = f\\n}\\n\\nfunc (m *MockedNodeConn) OnPullConfirmedOutput(f func(addr ledgerstate.Address, outputID ledgerstate.OutputID)) {\\n\\tm.onPullConfirmedOutput = f\\n}\\n\\nfunc (m *MockedNodeConn) OnPostTransaction(f func(tx *ledgerstate.Transaction)) {\\n\\tm.onPostTransaction = f\\n}\\n',\n",
       "   'role': 'user'}],\n",
       " 'generate_time': 20.40785813331604,\n",
       " 'completion': 'Here is a simple example of how you can write unit tests for the `MockedNodeConn` struct using the `testing` package in Go.\\n\\n```go\\npackage testchain\\n\\nimport (\\n\\t\"testing\"\\n\\t\"github.com/iotaledger/goshimmer/packages/ledgerstate\"\\n)\\n\\nfunc TestMockedNodeConn(t *testing.T) {\\n\\tmockedNodeConn := NewMockedNodeConnection(\"testID\")\\n\\n\\t// Test ID method\\n\\tif mockedNodeConn.ID() != \"testID\" {\\n\\t\\tt.Errorf(\"Expected ID to be \\'testID\\', got %s\", mockedNodeConn.ID())\\n\\t}\\n\\n\\t// Test PullBacklog method\\n\\texpectedAddr := &ledgerstate.AliasAddress{}\\n\\tmockedNodeConn.PullBacklog(expectedAddr)\\n\\tif mockedNodeConn.onPullBacklog == nil {\\n\\t\\tt.Errorf(\"Expected onPullBacklog to be set, got nil\")\\n\\t} else if mockedNodeConn.onPullBacklog(expectedAddr) != nil {\\n\\t\\tt.Errorf(\"onPullBacklog should not return anything\")\\n\\t}\\n\\n\\t// Test PullState method\\n\\texpectedAddr = &ledgerstate.AliasAddress{}\\n\\tmockedNodeConn.PullState(expectedAddr)\\n\\tif mockedNodeConn.onPullState == nil {\\n\\t\\tt.Errorf(\"Expected onPullState to be set, got nil\")\\n\\t} else if mockedNodeConn.onPullState(expectedAddr) != nil {\\n\\t\\tt.Errorf(\"onPullState should not return anything\")\\n\\t}\\n\\n\\t// Test PullConfirmedTransaction method\\n\\texpectedAddr = ledgerstate.Address{}\\n\\texpectedTxID := ledgerstate.TransactionID{}\\n\\tmockedNodeConn.PullConfirmedTransaction(expectedAddr, expectedTxID)\\n\\tif mockedNodeConn.onPullConfirmedTransaction == nil {\\n\\t\\tt.Errorf(\"Expected onPullConfirmedTransaction to be set, got nil\")\\n\\t} else if mockedNodeConn.onPullConfirmedTransaction(expectedAddr, expectedTxID) != nil {\\n\\t\\tt.Errorf(\"onPullConfirmedTransaction should not return anything\")\\n\\t}\\n\\n\\t// Test PullTransactionInclusionState method\\n\\texpectedAddr = ledgerstate.Address{}\\n\\texpectedTxID = ledgerstate.TransactionID{}\\n\\tmockedNodeConn.PullTransactionInclusionState(expectedAddr, expectedTxID)\\n\\tif mockedNodeConn.onPullTransactionInclusionState == nil {\\n\\t\\tt.Errorf(\"Expected onPullTransactionInclusionState to be set, got nil\")\\n\\t} else if mockedNodeConn.onPullTransactionInclusionState(expectedAddr, expectedTxID) != nil {\\n\\t\\tt.Errorf(\"onPullTransactionInclusionState should not return anything\")\\n\\t}\\n\\n\\t// Test PullConfirmedOutput method\\n\\texpectedAddr = ledgerstate.Address{}\\n\\texpectedOutputID := ledgerstate.OutputID{}\\n\\tmockedNodeConn.PullConfirmedOutput(expectedAddr, expectedOutputID)\\n\\tif mockedNodeConn.onPullConfirmedOutput == nil {\\n\\t\\tt.Errorf(\"Expected onPullConfirmedOutput to be set, got nil\")\\n\\t} else if mockedNodeConn.onPullConfirmedOutput(expectedAddr, expectedOutputID) != nil {\\n\\t\\tt.Errorf(\"onPullConfirmedOutput should not return anything\")\\n\\t}\\n\\n\\t// Test PostTransaction method\\n\\texpectedTx := &ledgerstate.Transaction{}\\n\\tmockedNodeConn.PostTransaction(expectedTx)\\n\\tif mockedNodeConn.onPostTransaction == nil {\\n\\t\\tt.Errorf(\"Expected onPostTransaction to be set, got nil\")\\n\\t} else if mockedNodeConn.onPostTransaction(expectedTx) != nil {\\n\\t\\tt.Errorf(\"onPostTransaction should not return anything\")\\n\\t}\\n}\\n```\\n\\nThis test suite covers the basic functionality of the `MockedNodeConn` struct. It checks if the methods are correctly setting and unsetting the corresponding callback functions, and if they are correctly calling the callback functions with the expected arguments.\\n',\n",
       " 'score_time': 0.5991272926330566,\n",
       " 'result': {'coverage': 0.0,\n",
       "  'error': 'go test build failed: stdout = {\"ImportPath\":\"github.com/iotaledger/wasp/packages/testutil/testchain [github.com/iotaledger/wasp/packages/testutil/testchain.test]\",\"Action\":\"build-output\",\"Output\":\"# github.com/iotaledger/wasp/packages/testutil/testchain [github.com/iotaledger/wasp/packages/testutil/testchain.test]\\\\n\"}\\n{\"ImportPath\":\"github.com/iotaledger/wasp/packages/testutil/testchain [github.com/iotaledger/wasp/packages/testutil/testchain.test]\",\"Action\":\"build-output\",\"Output\":\"packages/testutil/testchain/m_llm_test.go:22:12: mockedNodeConn.onPullBacklog(expectedAddr) (no value) used as value\\\\n\"}\\n{\"ImportPath\":\"github.com/iotaledger/wasp/packages/testutil/testchain [github.com/iotaledger/wasp/packages/testutil/testchain.test]\",\"Action\":\"build-output\",\"Output\":\"packages/testutil/testchain/m_llm_test.go:31:12: mockedNodeConn.onPullState(expectedAddr) (no value) used as value\\\\n\"}\\n{\"ImportPath\":\"github.com/iotaledger/wasp/packages/testutil/testchain [github.com/iotaledger/wasp/packages/testutil/testchain.test]\",\"Action\":\"build-output\",\"Output\":\"packages/testutil/testchain/m_llm_test.go:36:17: invalid composite literal type ledgerstate.Address\\\\n\"}\\n{\"ImportPath\":\"github.com/iotaledger/wasp/packages/testutil/testchain [github.com/iotaledger/wasp/packages/testutil/testchain.test]\",\"Action\":\"build-output\",\"Output\":\"packages/testutil/testchain/m_llm_test.go:41:12: mockedNodeConn.onPullConfirmedTransaction(expectedAddr, expectedTxID) (no value) used as value\\\\n\"}\\n{\"ImportPath\":\"github.com/iotaledger/wasp/packages/testutil/testchain [github.com/iotaledger/wasp/packages/testutil/testchain.test]\",\"Action\":\"build-output\",\"Output\":\"packages/testutil/testchain/m_llm_test.go:46:17: invalid composite literal type ledgerstate.Address\\\\n\"}\\n{\"ImportPath\":\"github.com/iotaledger/wasp/packages/testutil/testchain [github.com/iotaledger/wasp/packages/testutil/testchain.test]\",\"Action\":\"build-output\",\"Output\":\"packages/testutil/testchain/m_llm_test.go:51:12: mockedNodeConn.onPullTransactionInclusionState(expectedAddr, expectedTxID) (no value) used as value\\\\n\"}\\n{\"ImportPath\":\"github.com/iotaledger/wasp/packages/testutil/testchain [github.com/iotaledger/wasp/packages/testutil/testchain.test]\",\"Action\":\"build-output\",\"Output\":\"packages/testutil/testchain/m_llm_test.go:56:17: invalid composite literal type ledgerstate.Address\\\\n\"}\\n{\"ImportPath\":\"github.com/iotaledger/wasp/packages/testutil/testchain [github.com/iotaledger/wasp/packages/testutil/testchain.test]\",\"Action\":\"build-output\",\"Output\":\"packages/testutil/testchain/m_llm_test.go:61:12: mockedNodeConn.onPullConfirmedOutput(expectedAddr, expectedOutputID) (no value) used as value\\\\n\"}\\n{\"ImportPath\":\"github.com/iotaledger/wasp/packages/testutil/testchain [github.com/iotaledger/wasp/packages/testutil/testchain.test]\",\"Action\":\"build-output\",\"Output\":\"packages/testutil/testchain/m_llm_test.go:70:12: mockedNodeConn.onPostTransaction(expectedTx) (no value) used as value\\\\n\"}\\n{\"ImportPath\":\"github.com/iotaledger/wasp/packages/testutil/testchain [github.com/iotaledger/wasp/packages/testutil/testchain.test]\",\"Action\":\"build-fail\"}\\n{\"Time\":\"2025-05-19T10:43:34.755383829+03:00\",\"Action\":\"start\",\"Package\":\"github.com/iotaledger/wasp/packages/testutil/testchain\"}\\n{\"Time\":\"2025-05-19T10:43:34.755418874+03:00\",\"Action\":\"output\",\"Package\":\"github.com/iotaledger/wasp/packages/testutil/testchain\",\"Output\":\"FAIL\\\\tgithub.com/iotaledger/wasp/packages/testutil/testchain [build failed]\\\\n\"}\\n{\"Time\":\"2025-05-19T10:43:34.755424312+03:00\",\"Action\":\"fail\",\"Package\":\"github.com/iotaledger/wasp/packages/testutil/testchain\",\"Elapsed\":0,\"FailedBuild\":\"github.com/iotaledger/wasp/packages/testutil/testchain [github.com/iotaledger/wasp/packages/testutil/testchain.test]\"}\\n, stderr =  <class \\'Exception\\'>',\n",
       "  'error_type': 'test_build_failed',\n",
       "  'mutation_score': 0.0,\n",
       "  'test_results': {'all_passed': 0,\n",
       "   'failed': 0,\n",
       "   'passed': 0,\n",
       "   'root_failed': 0,\n",
       "   'root_passed': 0}},\n",
       " 'reward': 0.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async def score_row(row: dict) -> dict:\n",
    "    start = time.time()\n",
    "    scorer = Scorer(row['project_path'], row['relative_package_path'], None, relative_file_path=row['relative_file_path'])\n",
    "\n",
    "    evaluation_result = await scorer.score(row['completion'])\n",
    "\n",
    "    reward = scorer.calculate_reward(evaluation_result)\n",
    "\n",
    "    return {\n",
    "        'score_time': time.time()-start,\n",
    "        'result': evaluation_result,\n",
    "        'reward': reward,\n",
    "    }\n",
    "\n",
    "results = {}\n",
    "for i, row in tqdm(enumerate(by_file_test_with_completions_ds), total=len(by_file_test_with_completions_ds)):\n",
    "    results[(row['project_path'], row['relative_package_path'], row['relative_file_path'])] = await score_row(row)\n",
    "\n",
    "by_file_test_scored_ds = by_file_test_with_completions_ds.map(lambda row: results[(row['project_path'], row['relative_package_path'], row['relative_file_path'])], num_proc=1)\n",
    "\n",
    "by_file_test_scored_ds.save_to_disk(DATA_DIR+'/by_file_test_scored_ds')\n",
    "\n",
    "print(by_file_test_scored_ds)\n",
    "by_file_test_scored_ds[0]               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_file_test_with_completions_qwen_ds = datasets.load_from_disk(DATA_DIR+'/by_file_test_with_completions_qwen_ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for i, row in tqdm(enumerate(by_file_test_with_completions_qwen_ds), total=len(by_file_test_with_completions_qwen_ds)):\n",
    "    results[(row['project_path'], row['relative_package_path'], row['relative_file_path'])] = await score_row(row)\n",
    "\n",
    "by_file_test_scored_qwen_ds = by_file_test_with_completions_qwen_ds.map(lambda row: results[(row['project_path'], row['relative_package_path'], row['relative_file_path'])], num_proc=1)\n",
    "\n",
    "by_file_test_scored_qwen_ds.save_to_disk(DATA_DIR+'/by_file_test_scored_qwen_ds')\n",
    "\n",
    "print(by_file_test_scored_qwen_ds)\n",
    "by_file_test_scored_qwen_ds[0]               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_file_test_with_completions_dophine_ds = datasets.load_from_disk(DATA_DIR+'/by_file_test_with_completions_dophine_ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for i, row in tqdm(enumerate(by_file_test_with_completions_dophine_ds), total=len(by_file_test_with_completions_dophine_ds)):\n",
    "    results[(row['project_path'], row['relative_package_path'], row['relative_file_path'])] = await score_row(row)\n",
    "\n",
    "by_file_test_scored_dophine_ds = by_file_test_with_completions_dophine_ds.map(lambda row: results[(row['project_path'], row['relative_package_path'], row['relative_file_path'])], num_proc=1)\n",
    "\n",
    "by_file_test_scored_dophine_ds.save_to_disk(DATA_DIR+'/by_file_test_scored_dophine_ds')\n",
    "\n",
    "print(by_file_test_scored_dophine_ds)\n",
    "by_file_test_scored_dophine_ds[0]               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Анализ результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = datasets.load_from_disk(DATA_DIR+'/by_file_test_scored_ds').to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['coverage'] = df['result'].apply(lambda x: float(x['coverage']))\n",
    "df['mutation_score'] = df['result'].apply(lambda x: float(x['mutation_score']))\n",
    "df['error_type'] = df['result'].apply(lambda x: str(x['error_type']))\n",
    "df['all_passed'] = df['result'].apply(lambda x: int(x['test_results']['all_passed']))\n",
    "df['is_error'] = df['error_type'] != ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error count 370\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "error_type\n",
       "test_build_failed    170\n",
       "completion_parse     133\n",
       "go_tool_cover         34\n",
       "other                 21\n",
       "get_deps               7\n",
       "goimports              5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('error count', len(df.query('is_error')))\n",
    "\n",
    "df.query('is_error')['error_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward 0.17719959840000002\n",
      "coverage 21.313846153846157\n",
      "mutation_score 0.29743661666666665\n",
      "all_passed_count 60\n"
     ]
    }
   ],
   "source": [
    "print('reward', df['reward'].mean())\n",
    "print('coverage', df.query('is_error == False')['coverage'].mean())\n",
    "print('mutation_score', df.query('all_passed == 1')['mutation_score'].mean())\n",
    "print('all_passed_count', len(df.query('all_passed == 1')['mutation_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
